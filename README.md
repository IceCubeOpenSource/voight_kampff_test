# voight_kampff_test
Sanity checks on sequences/histograms using unsupervised machine learning.

# Dependencies

Currently the only dependency is numpy and optionally matplotlib for rendering histograms.

# Abstract
## Continuous Benchmarking with Machine Learning
Nearly all modern HPC applications execute in a distributed system.  Whether the system is a commercial cloud, public grid, or private cluster, application execution represents a significant cost of time, energy, and money.  Continuous Integration (CI) systems have been around for a while as the first stage of code validation.  More recently Continuous Delivery (CD) systems have become more popular, as a means to autonomously deliver validated software.  A new stage in this pipeline is emerging, called Continous Benchmarking (CB) that attempts to look beyond traditional unit and integration tests common to all CI/CD systems.  High level results of complex software chains are often difficult to test using traditional methods (i.e. unit/integration/system tests). This paper presents a technique used by the IceCube South Pole Neutrino Observatory in an autonomous system that uses traditional test statistics as inputs to a machine learning algorithm (specifically anomaly/outlier detection algorithm) to achieve broad coverage of its physics software stack, from initial event generation through detector simulation up to high levels of filtering (used to reject background and retain signal).  This State of Practice (SOP) paper will show that this technique offers several advantages (and often similar performance) over traditional statistical comparsions, such as Chi^2, Kolmogorov-Smirnoff, Anderson-Darling, log Likelihood, etc... in its broad and generic application.  No prior knowledge of distribution expectations are required.  Distributions are not required to be mathematically well-behaved, i.e. derived by sampling functions with continuous low-order derivatives.  Distributions are not required to adhere to  Poissonian statistics.  Finally, there is no threshold on the sample statistics required.  The technique presented is currently being used by IceCube in a nightly CI/CD/CB system to validate its physics codebase (consisting of >100 projects and 1M LoC) by comparing more than 10k distributions, generated at various stages in the simulation, reconstruction, and filtering pipeline.
